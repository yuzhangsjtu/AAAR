# 从聊天机器人到通用智能

2016 年，微软的聊天机器人 Tay 在 Twitter 上线不到 24 小时就被迫关闭。它没有任何判断能力，被网友诱导学会了种族歧视和阴谋论，然后忠实地把这些内容复述给全世界。同年，Google 的邮件智能回复功能把「I love you」列为几乎所有邮件的候选回复——不管对方是你的导师还是期刊编辑。

那时候的「AI 对话」大致是这个水平。它不理解你在说什么，也不在乎。

七年后的 2023 年，你把一篇写了一半的论文草稿贴给 GPT-4。它不仅读懂了文章在做什么，还指出了识别策略中一个你没注意到的问题——工具变量的外生性假设可能因为一个遗漏变量而受到威胁。它用的措辞像一位审稿人：克制、精准、直击要害。甚至你没有明说的研究动机——你为什么选择这个而不是那个识别策略——它也「读」出来了。

从「I love you」到精准的方法论批评，只隔了七年。但这七年发生的事，比人工智能前五十年加起来都多。这一章要讲的就是：到底发生了什么，以及这对你——一个在 AI 时代做研究的人——意味着什么。


## 一台预测下一个词的机器

所有现代 AI 的惊人表现，都建立在一个简单得有些不可思议的原理上：预测下一个词。

你的手机输入法每天都在做这件事。你打出「今天天气」，它建议「不错」或「很好」。它不理解天气，不关心你的心情，只是根据它见过的大量文本，判断「今天天气」后面最可能出现的词。

大语言模型（Large Language Model, LLM）做的事情本质上完全一样——预测给定上文之后，下一个最可能出现的词元（token）。只不过它的「上文」可以长达几万甚至几百万个词元，它的训练数据涵盖了互联网上能找到的几乎所有公开文本，而它的参数量从最初的一亿级别飙升到了千亿级别。

规模的变化在某个点上引发了质变。让一个小模型预测「今天天气」之后的词，它只能给出「不错」之类的高频搭配。让一个在全人类文本上训练的超大模型预测「在控制了混淆变量之后，处理效应的估计值......」之后的内容，它给出的不是一个词，而是一段在方法论上站得住脚的论述——因为它在训练中见过成千上万篇计量经济学论文，学会了这类文本的内在模式。

这就引出了一个深刻的理论洞见。DeepMind 的研究员 Marcus Hutter 早在 2005 年就提出了一个看似激进的论断：**压缩能力等于智能** [@hutter2005universal]。他设立了 Hutter Prize，悬赏能更好地压缩维基百科文本的算法。理由直截了当：要把一段文本压缩到最短，你必须发现其中所有的规律和冗余；而发现规律，就是智能的核心。

OpenAI 的联合创始人 Ilya Sutskever 将这个思想和语言模型直接联系起来：真正好的下一个词预测——真正好的压缩——会「发现数据中隐藏的秘密」。

这不是一个文学比喻，而是一个数学事实。信息论告诉我们，最优预测和最优无损压缩在数学上是等价的。一个能准确预测下一个词的模型，在本质上就是在压缩人类知识——它必须「理解」文本中的结构才能做到这一点。2024 年发表在 COLM 会议上的一项研究提供了直接的实证支持：在多个标准基准测试上，语言模型的文本压缩能力与它在各种「智能」测试上的表现呈线性正相关 [@huang2024compression]。换句话说，压缩做得越好的模型，在知识问答、推理、代码生成等任务上也表现得越好。

但请注意一个关键的区别：**这里的「理解」是统计意义上的，不是认知意义上的。** 模型学到的是「什么样的文字倾向于出现在什么样的上下文中」，而不是「这些文字描述的世界到底是什么样的」。一个模型可以写出完美的因果推断方法论描述，但它不像一个统计学家那样真正理解因果关系——它理解的是关于因果推断的文本长什么样。

这个区别在大多数实际使用中不影响效果。但在关键时刻——当训练数据中的模式和真实世界的事实发生偏离时——它就是幻觉和错误的根源。我们在下一章会详细讨论这一点。


::: {.callout-note}
## 什么是 Transformer？

2017 年，Google 的八位研究员发表了一篇标题颇为自信的论文：*Attention Is All You Need*（注意力就是你需要的全部）[@vaswani2017attention]。这篇论文提出了 Transformer 架构，解决了困扰自然语言处理领域多年的问题：如何高效处理文本中远距离词语之间的关系——比如一个长句子开头的主语和结尾的动词之间的对应。

Transformer 的核心是「自注意力机制」（self-attention）：让文本中的每个词都能「看到」其他所有词，并根据相关性分配不同的注意力权重。这使得模型不需要像之前的循环神经网络（RNN）那样逐词处理文本，而是可以并行处理整段内容，训练速度因此大幅提升。Transformer 是当今所有主流大语言模型——GPT、Claude、Gemini、Llama——的共同基础架构。

这篇论文至今被引用超过 17 万次。而八位作者后来全部离开了 Google，分别创立了估值数十亿美元的科技公司。其中 Noam Shazeer 在 2024 年以 27 亿美元的交易回到 Google 领导 Gemini 项目——原因是他 2021 年离开 Google 时，公司拒绝发布他开发的聊天机器人。这大概是科技史上最昂贵的「我早说过吧」。
:::


## 从互联网文本到有用的助手

理解了「预测下一个词」这个核心机制之后，一个自然的问题浮现出来：一台预测机器是怎么变成一个看起来善解人意的对话助手的？

答案藏在训练流程的两个截然不同的阶段中。

**第一阶段：预训练（Pre-training）——让模型读遍互联网。** 这是最耗时、最昂贵的阶段。模型在互联网上能找到的几乎所有公开文本上训练：维基百科、学术论文、新闻报道、GitHub 代码仓库、Reddit 论坛、数字化书籍......规模是人类个体无法想象的。GPT-3 的训练数据包含将近 5000 亿个词元 [@brown2020language]，一个人如果每天阅读 8 小时、一年读 365 天，大约需要两万年才能读完同等量的文本。

预训练的目标极其简单：给定一段文字，预测下一个词元。就是这么一个机械的目标，在万亿次重复之后产生了惊人的副产品——模型学会了语法规则、常识推理、各学科的知识体系、不同语言之间的对应关系，甚至学会了幽默和反讽。

但预训练出来的模型并不好用。你问它「什么是工具变量？」，它不会给你一个简洁的解答，而是会继续生成看起来像教科书或论文的段落——可能会再问几个问题，然后引出一段文献综述，因为这才是训练数据中「什么是工具变量」后面最常出现的文本模式。预训练模型学会了「世界上的文本长什么样」，但没有学会「当人类问问题时，我应该怎么回答」。

**第二阶段：后训练（Post-training）——让模型学会对话。** 这个阶段包含两个关键步骤，彻底改变了模型的行为模式。

第一步是**监督微调**（Supervised Fine-Tuning, SFT）。人类标注员编写大量「指令 → 高质量回答」的示范——比如「用通俗语言解释 p 值」→ 一段清晰准确的解释。模型在这些示范上训练，学会了一种新的行为模式：「当用户给我一个指令或问题时，我应该提供有帮助的回答。」

第二步是**基于人类反馈的强化学习**（Reinforcement Learning from Human Feedback, RLHF）。模型对同一个问题生成多个不同的回答，然后由人类标注员评判哪个更好、哪个更差。模型根据这些偏好信号调整自己的生成策略，逐渐学会产出人类更满意的回答 [@ouyang2022training]。

RLHF 解释了很多你可能已经注意到的 AI 行为特征。

为什么 AI 的回答总是那么「有帮助」、那么「礼貌」，有时甚至有点「讨好」？因为在 RLHF 训练中，标注员倾向于给态度友好、结构清晰的回答打高分。模型学会了：开头表示理解你的问题，中间分点论述，结尾总结要点——因为这种格式的得分最高。

为什么 AI 有时会在不确定的问题上表现得过于自信？因为在人类评分中，一个自信流畅但包含错误的回答，往往比一个坦承「我不确定」的回答得分更高——标注员容易被文字的流畅性和表面上的完整性所蒙蔽。

这不是某家公司的疏忽，而是当前训练范式的一个结构性特征：**AI 被优化来产生让人满意的回答，而非正确的回答。** 大多数情况下，「让人满意」和「正确」是一致的。但当二者冲突时——比如一个问题模型没有可靠答案但需要「看起来有帮助」——AI 会倾向于生成一个「看起来对」的回答，而不是说「我不知道」。


::: {.callout-warning}
## 预训练数据的截止日期

大语言模型的知识有一个硬性边界：预训练数据的截止日期。截止日期之后发生的事，模型完全不知道——不是它「忘了」，而是这些信息从未存在于它的训练数据中。

如果你问一个训练数据截止到 2024 年的模型「2025 年的诺贝尔经济学奖得主是谁」，它要么诚实地告诉你不知道，要么——更危险地——编造一个听起来合理的答案。后者的危险在于，模型编造的答案通常非常像真的：它会选择一个确实在诺贝尔奖热门名单上的经济学家，配上一个听起来合理的获奖理由，让你很难仅凭直觉识别出这是编造的。

部分工具通过联网搜索来缓解这个问题，但搜索结果的质量和整合方式参差不齐。一个实用的习惯：任何涉及时效性的事实——最新研究进展、当前政策、工具的最新版本、近期发生的事件——都需要独立验证，不能依赖 AI 的直接回答。
:::


## 上下文：AI 的工作记忆

理解了 AI 的训练流程之后，还需要理解它在对话中的运作方式——特别是一个被很多人误解的概念：记忆。

和人类不同，AI 没有真正的记忆。每一次你发送消息，系统会把整个对话历史——你说的每一句话、AI 回的每一段话——拼接成一个长文本，完整地输入模型。这个文本的最大长度，就是**上下文窗口**（context window）。

上下文窗口的大小决定了 AI 能在一次对话中「记住」多少内容。2023 年初 GPT-4 刚发布时，上下文窗口是 8000 个词元，大约相当于 4000 个中文字——差不多一篇短论文的长度。到 2025 年，主流模型的上下文窗口已经扩展了几个数量级：Claude 支持 20 万个词元（约 10 万中文字），Google 的 Gemini 达到了 200 万个词元，Meta 的 Llama 4 甚至声称支持 1000 万个词元。

这意味着你现在可以把一整本教科书或几十篇论文一次性输入 AI，让它在全部内容的基础上回答问题。听起来很美好，但有三个容易被忽视的事实。

**第一，每一轮对话都会重发全部历史。** 你以为自己在和 AI「持续聊天」，实际上每次你发一条新消息，系统都把从对话开头到当前为止的所有内容完整地再送一遍给模型。这意味着越往后聊，每一轮消耗的词元越多，成本越高，延迟也越大。如果你在一个长对话中感觉 AI 变慢了或者回答质量下降了，原因不是它「疲倦」了，而是上下文已经接近窗口上限，模型在处理大量信息时效率降低。

**第二，对话之间没有记忆。** 你今天和 AI 深入讨论了你的研究方向、方法论偏好、写作风格，关掉对话之后，这些信息就消失了。明天开一个新对话，它对你一无所知。一些平台开始提供所谓的「记忆」功能——把之前对话的关键信息存下来——但这些功能目前还很初级，本质上是把压缩后的笔记塞进新对话的上下文窗口中，并非真正的学习和记忆。

**第三，上下文越长，成本增长越快。** AI 工具按输入和输出的词元数量计费。输出（AI 生成的内容）通常比输入（你给 AI 的内容）贵 3 到 5 倍。一次简短的问答花费微乎其微，但如果你把 50 篇论文塞进上下文让它做综述，成本会迅速累积——不仅因为输入的量大，还因为超长上下文的请求本身就有额外加价。

理解上下文机制对高效使用 AI 有直接指导意义：把关键背景信息（你的研究问题、需要遵循的格式、重要的约束条件）放在对话开头而不是中间；在长对话出现质量下降时，与其继续追问，不如总结前文要点、开一个新对话重新开始；不要对 AI 的「个性化」能力抱有不切实际的期待——每个新对话都是一个全新的开始。


::: {.callout-tip}
## 词元（Token）不是字

你可能注意到我一直用「词元」（token）而不是「字」或「词」。这不只是术语选择——它直接影响你对 AI 工具成本和能力的理解。

语言模型不按人类理解的「词」来处理文本，而是按 token——一种模型自己从数据中学习出来的文本切分单位。英文中，一个常见单词通常是一个 token（如 the、research），但不常见的长词会被拆成多个 token。中文的情况更复杂：一个常用汉字一般对应 1 到 2 个 token，但某些生僻字或专业术语可能消耗更多。

粗略估算：1000 个中文字约等于 500-700 个 token。一篇 8000 字的论文大约是 4000-5000 个 token。以当前主流模型的价格计算，让 AI 读完这篇论文并写出一段摘要，总费用不到一美分。对于个人使用来说，成本几乎可以忽略。但如果你在做一个需要处理上千篇文献的系统综述项目，成本就会成为一个需要认真考虑的因素。
:::


## 从聊天到行动：AI 的五个级别

到目前为止，我们讨论的 AI 本质上还是一个「对话伙伴」：你问，它答。但 AI 的发展方向远不止于此。

2024 年 7 月，OpenAI 在内部会议上披露了一个五级框架，用来衡量 AI 向通用人工智能（Artificial General Intelligence, AGI）推进的程度。这个框架后来被广泛引用，因为它提供了一个简洁的坐标系来定位当前 AI 的位置：

| 级别 | 名称 | 能力描述 |
|------|------|----------|
| 1 | 聊天机器人（Chatbots） | 能进行自然语言对话 |
| 2 | 推理者（Reasoners） | 具备博士水平的问题解决能力 |
| 3 | 智能体（Agents） | 能自主执行跨步骤的复杂任务 |
| 4 | 创新者（Innovators） | 能辅助发明和科学发现 |
| 5 | 组织（Organizations） | 能完成一整个组织的工作 |

2024 年 9 月，OpenAI 发布了 o1 推理模型后，宣布 AI 已经达到了第二级。Sam Altman 在谈到 o1 时做了一个意味深长的比较：他说 o1 之于推理模型，就像 GPT-2 之于语言模型——暗示真正的「GPT-4 级别」推理能力还在后面。

这个框架中有一条至关重要的分界线，值得每个研究者理解：**聊天机器人和智能体的区别。**

聊天机器人（Chatbot）是一个**回应者**。你给它一个问题，它生成一段回答。你不问，它什么也不做。哪怕它的回答再精彩，它的能力本质上局限在「生成文本」这一个动作上。

智能体（Agent）是一个**行动者**。你给它一个目标而不是一个问题，它自己分解任务、规划步骤、调用工具、执行操作、处理异常情况。你说「帮我找到过去五年关于 XX 主题的所有系统综述，筛选出方法论评分在 8 分以上的，整理成一个对比表格」，一个真正的智能体能自主完成整个流程：搜索学术数据库、下载论文、阅读并评估质量、生成结构化的对比表——不需要你在每一步都给出指令。

你现在日常使用的 AI 工具，大多处于聊天机器人和智能体之间的模糊地带。ChatGPT 能联网搜索、运行 Python 代码、调用 DALL-E 生成图片；Claude 能阅读上传的文档、处理复杂的分析任务。这些已经超越了纯聊天机器人的范畴——它们不只是生成文本，而是在文本生成的基础上使用了外部工具。但它们离真正的智能体还有明确的距离：真正的智能体需要在多个步骤之间保持连贯的规划、处理执行中的意外和失败、在不确定的情况下自主做出判断——而不仅仅是一步步响应用户的指令。

对研究者来说，这个区分有直接的实际意义。当你面前的 AI 是一个聊天机器人时，风险边界是清晰的——它只做你让它做的事，你能在每一步检查输出。当它开始表现得像一个智能体——自动搜索文献、自动筛选结果、自动做出某些判断——你就需要额外的警惕：它在哪些环节自主做了决定？这些决定你同意吗？它有没有遗漏什么？

框架中的第 4 级「创新者」和第 5 级「组织」目前还是愿景而非现实。但从第 1 级到第 3 级的跃迁正在发生——理解这个跃迁，是为了在它真正全面发生时，你已经建立了足够的判断力来驾驭它。


## 站在顶点的人怎么看

了解 AI 的技术机制是一回事，了解创造这项技术的人如何看待自己的创造物，是另一回事。AI 领域最具影响力的几位人物，对 AI 的未来有着截然不同的判断——这些分歧本身就蕴含着重要的信息。

**Geoffrey Hinton** 被称为「AI 教父」——他在 1980 年代坚持研究神经网络（neural network），那时候整个学术界认为这条路走不通。四十年后，他的执着获得了最高认可：2024 年诺贝尔物理学奖。一个计算机科学家获得物理学奖，本身就是 AI 跨界影响力的象征。

但 Hinton 在诺贝尔晚宴的致辞中没有庆祝，而是发出了警告：

> 当我们创造出比我们更聪明的数字存在时，长期的存在性威胁也会随之而来。我们不知道能否维持控制。但我们现在有证据表明，如果它们是由追求短期利润的公司创造的，我们的安全不会是最优先考虑的事项。

2023 年，他从工作了十年的 Google 辞职，为的是能自由讨论 AI 的风险而不用顾虑对前雇主的影响。他估计，在未来 5 到 20 年内，有「50% 的概率」出现比人类更聪明的 AI。一位把毕生精力投入到让 AI 变得更强大的科学家，到了职业生涯的尾声却认为自己创造的东西可能是一个存在性威胁——这个转变本身就值得深思。

**Ilya Sutskever** 走了一条更加戏剧性的路。作为 OpenAI 的联合创始人和首席科学家，他在 2023 年 11 月参与了那场震动整个科技界的事件——OpenAI 董事会短暂解雇了 CEO Sam Altman。事件平息后，Sutskever 在 2024 年中离开 OpenAI，创立了一家名为 Safe Superintelligence Inc.（SSI）的公司。这家公司只做一件事：研究如何安全地实现超级智能。到 2025 年，SSI 在没有任何产品和收入的情况下，估值达到 320 亿美元。这个数字不反映任何商业价值，它反映的是 AI 领域对特定个人判断力的极端信赖。

Sutskever 最具洞察力的观点是关于 AI 发展瓶颈的判断。他认为 AI 的「缩放时代」（scaling era）——通过不断增加数据、参数和计算量来提升模型性能的阶段——正在接近尾声。下一个突破不会来自更多的 GPU，而是来自「对学习本身的更深理解」。他的原话是：「AI 的瓶颈是思想，不是算力。」

如果 Sutskever 是对的，意味着 AI 的进步路径将变得不可预测。过去十年，AI 的能力沿着 GPT-1 → GPT-2 → GPT-3 → GPT-4 的路线大致平滑地推进，研究者可以合理推断下一代模型「大概比上一代好多少」。但如果未来的突破来自某个全新的算法洞见而非规模扩张，那么进步可能在某个不可预见的时间点突然发生跳变——就像 Transformer 在 2017 年的出现一样，一篇论文改变了一切。

Hinton 担心 AI 太强大了。Sutskever 也担心 AI 太强大，但他选择从内部解决问题而不是从外部呼吁。OpenAI 在「构建通用智能」和「确保安全」之间持续拉锯。Google、Meta、Anthropic 各有各的路线和各自的焦虑。

这些分歧告诉我们一件重要的事：**即使是最顶尖的 AI 研究者，也不确定 AI 正在走向哪里。** 任何声称能确定预测 AI 未来的人——无论是「AI 将在三年内取代所有白领工作」还是「AI 只是一个更好的搜索引擎」——都在表达信仰，而非陈述事实。

如果创造 AI 的人都无法达成共识，那么作为 AI 的使用者，我们唯一能依赖的就是自己的判断力。而建立判断力的前提，是理解这项技术到底是什么、能做什么、不能做什么——这正是本书第一部分要做的事。

::: {.callout-note}
## GPT 的命名与演化

GPT 是 Generative Pre-trained Transformer 的缩写——生成式预训练 Transformer。这个名字精确地描述了技术本身：基于 Transformer 架构、通过预训练学习、以生成文本为核心能力。

GPT-1（2018 年，1.17 亿参数）证明了预训练 + 微调这条路径是可行的。GPT-2（2019 年，15 亿参数）展示了零样本学习的可能性——不需要微调就能完成一些任务。OpenAI 当时认为 GPT-2「太危险而不能发布」，这在今天看来几乎是一个笑话，但它标志着 AI 安全讨论从学术角落走向了公众视野。GPT-3（2020 年，1750 亿参数）引入了少样本学习（few-shot learning），只需要在提示中给几个示例就能让模型学会新任务 [@brown2020language]。从 GPT-1 到 GPT-3，参数量增长了 1500 倍，而每一次规模跳跃都伴随着意料之外的能力涌现。

GPT-4（2023 年）之后，OpenAI 不再公开参数量，竞争的焦点从「谁的模型更大」转向了「谁的训练数据更好、对齐做得更精细、架构更高效」。模型大小不再是决定性因素——就像汽车工业在某个阶段从比拼马力转向了比拼操控和安全一样。
:::


## 从这里开始

这一章勾勒了一幅全景：AI 是一台预测下一个词的机器，通过海量数据的预训练获得了广博的「知识」，通过后训练学会了以对话的方式提供帮助，正在从被动的聊天机器人向主动的智能体演化。创造它的人对它的未来看法不一，但有一点是共识：这项技术正处于快速且不可预测的变化之中。

如果你只从这一章带走一个核心认知，应该是这个：**AI 的所有强项和所有问题，都可以从「在海量文本上预测下一个词」这个简单机制中推导出来。** 它的优势——流畅的语言、广博的知识、高效的模式识别——来自于训练数据中的模式确实存在且大多数时候可靠。它的弱点——幻觉、推理漏洞、缺乏真正的创造力——来自于模式匹配不等于理解，统计上合理不等于事实上正确。

接下来的三章将把这个认知展开为实用的判断力。下一章「锯齿状的智能」会深入分析 AI 的能力边界——不是笼统地说「AI 有时会犯错」，而是拆解它在哪类场景下最容易出错、错误的根源是什么、以及作为研究者你如何识别和防范这些风险。第三章「遍地神灯的时代」回应一个更实际的问题：面对不断涌现的新工具和新模型，你该如何评估和选择。第四章「思考不能外包」讨论一个更根本的问题：即使 AI 能替你做某件事，你是否应该让它做——以及哪些思考过程本身就是做研究的意义所在。

理解了 AI 是什么，才能判断 AI 能做什么。而判断力——不是工具技巧——才是 AI 时代做好研究的真正护城河。
